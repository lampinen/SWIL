\documentclass{article}

\usepackage{url}            
\usepackage{booktabs}       
\usepackage{amsfonts}       
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{nicefrac}       
\usepackage{microtype}      
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{natbib}
\usepackage[margin=1in]{geometry}

\newcommand{\R}{\mathbb{R}}

\begin{document}
\section{Theoretical appendix}
\subsection{Preliminaries}
\citet{Saxe2014} provided (in part A of their supplementary material) solutions to learning dynamics in linear networks from arbitrary initial mode strengths, with some assumptions about the initialization and the structure of the data. We note a minor correction to these solutions and build off them here. \par
Consider the input-output correlation matrix $\Sigma_{31} = \sum_{i=1}^P y_ix_i^t$ where $\{(x_1, y_), ..., (x_P, y_P)\}$ are the (input, target) pairs the network is trained on. Saxe and colleagues considered its singular value decomposition:
$$\Sigma_{31} = \sum_{\alpha=1}^{k} u_\alpha s_\alpha v_\alpha^T$$
Saxe and colleagues assumed that the input-input correlation matrix is white ($\Sigma_11 = \sum_{i=1}^P x_i x_i^t = I$), and under the assumption that the network is initialized so that the singular value modes are decoupled, they showed that the modes then remained decoupled and gave exact solutions for the learning of these modes from small initial weights. In the supplementary material, they also expanded this to arbitrary initial weight size (but still assuming decoupled initialization). \par
Specifically, consider singular mode $i$. For ease of explanation, we change the basis of the representational layer of the network so each mode is represented by a single hidden unit -- this is permissable because we assumed the modes were decoupled. (This is equivalent to the change of variables denoted by bars by Saxe and colleagues.) Let the initial projection of this unit's input weights onto the input mode $v_i$ be $a(0)$, and the initial projection of its output weights onto the output mode $u_i$ be $b(0)$. Saxe and colleagues showed that $(a(t), b(t))$ evolve over time along hyberolas of constant $a^2-b^2$ until they approache $ab = s_i$, i.e. the true strength of that mode in the data. \par
Without loss of generality we assume $a(0) + b(0) > 0$ (the other half-space requires a trivial reparameterization). We can then parameterize this hyperbola by the angle $\theta$ and make the change of variables
$$a = \sqrt{2c_0} \cosh \frac{\theta}{2}, \qquad b = \sqrt{2c_0} \sinh \frac{\theta}{2}$$
Where $c_0 = 2 (a(0)^2-b(0)^2)$ so that
$$ab = c_0 \sinh \theta$$
Following their derivation with this change of variables, we arrive at:
$$\frac{\tau}{2} \frac{d\theta}{dt} = s_i - c_0 \sinh \theta$$
(the factor of two can also be absorbed into the time constant $\tau$, we leave it separate here to avoid changing the definition of $\tau = 1/\lambda$ from the original paper). \par
This differential equation is seperable, and so we can solve for the time needed to traverse along the hyperbola from an initial point $\theta_0$ to a final point $\theta_f$:
$$t = \frac{\tau}{2\sqrt{c_0^2 + s_i^2}} \left[\ln \frac{\sqrt{c_0^2 + s_i^2} + c_0 + s_i \tanh \frac{\theta}{2}}{\sqrt{c_0^2 + s_i^2} - c_0 - s_i \tanh \frac{\theta}{2}}\right]_{\theta_0}^{\theta_f}$$
This provides an exact analytic solution for the time a given degree of learning requires. Although this equation cannot be analytically inverted to find $\theta(t)$ (and thereby $a(t)$ and $b(t)$), we can parametrically sweep through the interval $(\theta_0, \theta_f)$ to plot the theoretical learning curve. \par
%%To aid interpretation, we reverse the substitution:
%%$$t = \frac{\tau}{2\sqrt{c_0^2 + s_i^2}} \left[\ln \frac{\sqrt{c_0^2 + s_i^2} + c_0 + s_i \frac{b}{a}}{\sqrt{c_0^2 + s_i^2} - c_0 - s_i \frac{b}{a}}\right]_{(a(0), b(0))}^{(a(t_f), b(t_f))}$$
We note that 
$$\frac{d(ab)}{dt} = a \frac{db}{dt} + b \frac{da}{dt} = 2 c_0 (s_i - c_0 \sinh \theta) \left( \cosh^2 \frac{\theta}{2} + \sinh^2 \frac{\theta}{2} \right) = 2 c_0 \left(s_i - ab\right) \left(a^2 + b^2 \right)$$
That is, the change in the network's representation of a mode is proportional both to how far the mode is from its correct value (i.e. the error) and to how (absolutely) large the two components of the mode are. The sigmoidal trajectory of mode learning noted by Saxe and colleagues can easily be interpreted from this equation -- with a small weight initialization, the second term is small, and so the change in the strength of the mode is small. As $a$ and $b$ increase, the rate of change increases, until $ab$ approaches its asymptotic value of $s_i$, at which point learning slows down as the error term shrinks. \par
\subsection{Learning from different starting points}
We are now in a position to examine the question of how new knowledge gets integrated into linear networks that have already learned something. Given the above, this reduces to the question of how this new knowledge projects onto the knowledge that is already stored in the network. Qualitatively, new knowledge which provides a minor adjustment of existing knowledge will be rapidly integrated, since the projections of the new singular dimensions onto the old singular dimesnions will be strong (i.e. the second term of $d(ab)/dt$ will be large), and the first term will be proportional to the amount of adaption needed. By contrast, entirely new knowledge (i.e. modes that are orthogonal to all previously learned modes) will be integrated quite slowly. In fact, it will be learned \textbf{over the same period of time as it would have taken to learn this mode in a randomly initialized network}, assuming the modes are decoupled. \color{red}{[simulations here]} 
\subsection{Learning multiple non-orthogonal new modes}



\end{document}
